{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cholera Outbreaks in Sub-Saharan Africa 2010-2019\n",
    "\n",
    "Let's look at outbreak data from the Infectious Disease Dynamics Group at Johns\n",
    "Hopkins University as given in their\n",
    "[GitHub repository of Cholera Outbreaks in Sub-Saharan Africa 2010-2019](https://github.com/HopkinsIDD/cholera_outbreaks_ssa).\n",
    "Specifically, we'll look at the outbreak reference data in\n",
    "`reference_data/outbreak_data.csv` within that repository, which we've\n",
    "downloaded to `data/outbreak_data.csv` within this repository.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from typing import Any\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by looking at a few rows of the outbreak data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outbreaks_df = pd.read_csv(\"data/outbreak_data.csv\")\n",
    "outbreaks_df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we have 999 rows with some missing values, but we'll address\n",
    "our numerical data later. Let's first see all of the columns we have:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in sorted(outbreaks_df.columns):\n",
    "    print(column)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We appear to have a few columns related to outbreak locations, so let's see if\n",
    "any of them provide any useful geocoding information:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outbreaks_df[[\"area\", \"country\", \"location\", \"who_region\"]]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `area` column appears to be the size of the area, which doens't help us with\n",
    "geocoding, and `who_region` doesn't really help us either, but the 3-letter ISO\n",
    "code for `country` and the parts within `location` should be all we need to\n",
    "obtain relevant GeoJSON data.\n",
    "\n",
    "Let's first look at the unique `country` values:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outbreaks_df[\"country\"].unique()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears we have at least 1 row where the country is `TZA_zanzibar` instead of\n",
    "simply `TZA`, but before we bother cleaning up the `country` values, according\n",
    "to the [data description](https://github.com/HopkinsIDD/cholera_outbreaks_ssa),\n",
    "the `location` column contains:\n",
    "\n",
    "> the name of the location where outbreak cases were reported and the name is\n",
    "> made up of WHO region, country, and administrative units seperated by \"::\".\n",
    "\n",
    "Therefore, the values in the `country` column appear to be duplicated in the\n",
    "`location` column. Since we want to split out the constituent parts of\n",
    "`location` anyway, let's do that, and perhaps just use the country component of\n",
    "`location` instead of the existing `country` column, if it does not suffer from\n",
    "the same problem.\n",
    "\n",
    "The WHO Region within `location`, not only already exists in the `who_region`\n",
    "column, but also doesn't help our geocoding effort, so we'll just drop that\n",
    "component.  That means we'll simply have standard administrative regions from\n",
    "`location`, starting with the country, which is administrative level 0, followed\n",
    "by successively smaller administrative units (1, 2, etc.):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "admins_df = (\n",
    "    outbreaks_df[\"location\"]\n",
    "    .str.split(\"::\", expand=True)  # Split `location` parts into columns\n",
    "    .drop([0], axis=1)  # Drop WHO region (column 0)\n",
    "    .rename(columns=lambda i: \"country\" if i == 1 else f\"adm{i - 1}_raw\")\n",
    "    .apply(lambda column: column.str.upper() if column.name == \"country\" else column)\n",
    ")\n",
    "\n",
    "admins_df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if our new `country` column suffers from the same problem as the\n",
    "existing `country` column:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "admins_df[\"country\"].unique()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, we can see that our new `country` column appears to be clean, so we\n",
    "don't need to clean it up.\n",
    "\n",
    "However, it appears that many rows do not have a value for `adm2` and `adm3`,\n",
    "so we likely need to group the data by `adm1`. We'll deal with such grouping\n",
    "later, but let's first confirm that every row has a value for `adm1`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(admins_df[\"adm1_raw\"])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, every row does have a value for `adm1`. Therefore, assuming\n",
    "they're all valid values (we'll clean up dirty data later, if necessary), we\n",
    "should simply need to fetch GeoJSON data for each of the distinct countries at\n",
    "administrative level 1, using the country ISO codes above (converted to upper\n",
    "case).\n",
    "\n",
    "We'll use the [geoboundaries API](https://www.geoboundaries.org/api.html) to\n",
    "fetch the GeoJSON files, but since these files can be quite large, we need to\n",
    "write the files to disk to avoid refetching the files each time we run this\n",
    "notebook:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geoboundary(iso3: str, level: int = 1) -> dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Return GeoJSON for a country at an administrative level from GeoBoundaries API.\n",
    "    \"\"\"\n",
    "    iso = iso3.upper()\n",
    "    path = f\".geoboundaries-cache/{iso}-ADM{level}.geojson\"\n",
    "\n",
    "    # If we've already downloaded this file, read it and return the contents.\n",
    "    if os.path.exists(path):\n",
    "        with open(path, \"r\") as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    # Fetch metadata from geoboundaries to obtain GeoJSON URL\n",
    "    url = f\"https://www.geoboundaries.org/api/current/gbOpen/{iso}/ADM{level}\"\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    # Extract value of `\"gjDownloadURL\"` from `metadata`, which is the GeoJSON URL,\n",
    "    # and download the GeoJSON file.\n",
    "    metadata = response.json()\n",
    "    response = requests.get(metadata[\"gjDownloadURL\"])\n",
    "    response.raise_for_status()\n",
    "\n",
    "    # Write the downloaded GeoJSON file to disk in case we need it later.\n",
    "    with open(path, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "    return response.json()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can fetch the administrative level 1 GeoJSON for every country in the\n",
    "dataset and look at the `country` and `adm1` values:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "official_adm1s_df = pd.DataFrame(\n",
    "    columns=[\"country\", \"adm1\"],\n",
    "    data=(\n",
    "        [country, feature[\"properties\"][\"shapeName\"]]\n",
    "        for country in admins_df[\"country\"].unique()\n",
    "        for feature in geoboundary(country)[\"features\"]\n",
    "    ),\n",
    ")\n",
    "\n",
    "official_adm1s_df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's one last issue to address. If we look at the `adm1` values in\n",
    "`admins_df`, we'll see that they don't quite line up with the `adm1` values in\n",
    "`official_adm1s_df`. Specifically, the values in `admins_df` are all\n",
    "lowercase, contain no whitespace (multi-word names are smashed together), and\n",
    "diacritics are removed. This means that we won't be able to readily look up the\n",
    "coordinates of a region, so we need to do a bit of work to support such lookups.\n",
    "\n",
    "We'll use `thefuzz` library to do some \"fuzzy\" matching for us:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thefuzz import process\n",
    "\n",
    "\n",
    "def append_adm1_match(row):\n",
    "    country, adm1_raw = row.loc[[\"country\", \"adm1_raw\"]]\n",
    "    choices = official_adm1s_df.query(f\"country == '{country}'\")[\"adm1\"]\n",
    "    adm1_match, *_ = process.extractOne(adm1_raw, choices, score_cutoff=70) or (None,)\n",
    "\n",
    "    return pd.concat([row, pd.Series({\"adm1_match\": adm1_match})])\n",
    "\n",
    "\n",
    "matched_adm1s_df = (\n",
    "    admins_df[[\"country\", \"adm1_raw\"]]\n",
    "    .drop_duplicates()\n",
    "    .sort_values([\"country\", \"adm1_raw\"])\n",
    "    .apply(append_adm1_match, axis=1)\n",
    ")\n",
    "\n",
    "matched_adm1s_df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are still a number of unmatched `adm1_raw` values, so let's see what they\n",
    "are and how many there are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unmatched_adm1s_df = matched_adm1s_df[matched_adm1s_df[\"adm1_match\"].isna()][\n",
    "    [\"country\", \"adm1_raw\"]\n",
    "].drop_duplicates()\n",
    "print(f\"Number of unique unmatched ADM1 values: {len(unmatched_adm1s_df)}\")\n",
    "unmatched_adm1s_df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that these `adm1_raw` values are actually names of administrative\n",
    "level 2 regions.  For example, Kampala is the capital city of Uganda, which is\n",
    "an administrative level 2 region since it is not a province.\n",
    "\n",
    "The next step is to obtain ADM2 GeoJSONs from the geoboundaries API to see if\n",
    "we can match these remaining `adm1_raw` values to ADM2 names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "official_adm2s_df = pd.DataFrame(\n",
    "    columns=[\"country\", \"adm2\"],\n",
    "    data=(\n",
    "        [country, feature[\"properties\"][\"shapeName\"]]\n",
    "        for country in unmatched_adm1s_df[\"country\"].unique()\n",
    "        for feature in geoboundary(country, 2)[\"features\"]\n",
    "    ),\n",
    ")\n",
    "\n",
    "official_adm2s_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_adm2_match(row):\n",
    "    country, adm1_raw = row.loc[[\"country\", \"adm1_raw\"]]\n",
    "    choices = official_adm2s_df.query(f\"country == '{country}'\")[\"adm2\"]\n",
    "    adm2_match, *_ = process.extractOne(adm1_raw, choices, score_cutoff=75) or (None,)\n",
    "\n",
    "    return pd.concat([row, pd.Series({\"adm2_match\": adm2_match})])\n",
    "\n",
    "\n",
    "matched_adm2s_df = (\n",
    "    unmatched_adm1s_df[[\"country\", \"adm1_raw\"]]\n",
    "    .drop_duplicates()\n",
    "    .sort_values([\"country\", \"adm1_raw\"])\n",
    "    .apply(append_adm2_match, axis=1)\n",
    ")\n",
    "\n",
    "matched_adm2s_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_adm2s_df[matched_adm2s_df[\"adm2_match\"].isna()]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have 2 remaining issues to address:\n",
    "\n",
    "1. We have 8 ADM1 values left that do not match ADM2 values.\n",
    "1. For all of the ADM1 values that are actually ADM2 values (22 - 8 = 14), we\n",
    "   have to find their parent ADM1 values.  Unfortunately, the GeoJSONs returned\n",
    "   by the geoboundary API do not have a way to find the parent ADM1 values."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo-ds-cholera",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
