{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imblearn\n",
    "import pandas as pd\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.read_csv(\"../data/zonal-means-aggregate-200910-201912.csv\")\n",
    "full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = full_df[\"outbreak\"].value_counts()\n",
    "print(\"Class Distribution:\\n\", class_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename target columns to focus on oversample of minority class\n",
    "X = full_df.drop(columns=[\"outbreak\", \"location_period_id\", \"year\"])\n",
    "y = full_df[\"outbreak\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SMOTE algorithm for treating imbalanced datasets cannot deal with missing values (NaNs) for Feature columns, so we need to impute the missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying simplist forward fill imputation strategy first\n",
    "X_imputed = X.ffill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the result back to a DataFrame\n",
    "X_imputed = pd.DataFrame(X_imputed, columns=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into train and test splits\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_imputed, y, test_size=0.3, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply SMOTE to the training data with a 1:10 ratio as used by Campbell et al 2020\n",
    "smote = SMOTE(sampling_strategy=0.1, random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the new class distribution after SMOTE\n",
    "resampled_class_counts = pd.Series(y_resampled).value_counts()\n",
    "print(\"\\nClass Distribution after SMOTE:\\n\", resampled_class_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train your machine learning model on the balanced dataset\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "clf.fit(X_resampled, y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate your model\n",
    "accuracy = clf.score(X_test, y_test)\n",
    "print(\"\\nModel Accuracy on Test Set:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train your machine learning model on the balanced dataset (already done in the previous code)\n",
    "\n",
    "# get feature importances from the trained RandomForestClassifier\n",
    "feature_importances = clf.feature_importances_\n",
    "\n",
    "# create a DataFrame to display feature names and their corresponding importances\n",
    "feature_importance_df = pd.DataFrame(\n",
    "    {\"Feature\": X_imputed.columns, \"Importance\": feature_importances}\n",
    ")\n",
    "\n",
    "# sort the DataFrame by importance in descending order\n",
    "feature_importance_df = feature_importance_df.sort_values(\n",
    "    by=\"Importance\", ascending=False\n",
    ")\n",
    "\n",
    "# print the top N most influential features (adjust N as needed)\n",
    "top_n_features = 10  # Change this to the number of top features you want to display\n",
    "print(f\"Top {top_n_features} Most Influential Features:\")\n",
    "print(feature_importance_df.head(top_n_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO: \n",
    "* Move model development to another notebook\n",
    "* Reduce (redundant or correlated) features through PCA or similar\n",
    "* Run sensitivity analysis on imputation strategies"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo-ds-cholera",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
