{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the Outbreak Geometries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "import warnings\n",
    "\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from shapely.errors import ShapelyDeprecationWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=ShapelyDeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll read our outbreak data and get outbreak counts per spatial scale:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outbreaks_df = pd.read_csv(\n",
    "    \"../data/outbreak_data.csv\", parse_dates=[\"start_date\", \"end_date\"]\n",
    ")\n",
    "outbreaks_df.value_counts(\"spatial_scale\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't want to work with outbreaks across different spatial scales because\n",
    "doing so might present issues with overlapping geometries that might cause\n",
    "problems for computing zonal statistics, so we'll select all outbreaks in admin2\n",
    "regions, since that represents the majority of the outbreaks.\n",
    "\n",
    "Further, we'll add `start_year`, `start_month`, and `duration_in_months` columns\n",
    "to show an example of how we can do this elsewhere (not in this notebook) to\n",
    "determine the full span of months for which we'll need to obtain environmental\n",
    "parameter data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "admin2_outbreaks_df = outbreaks_df.query(\"spatial_scale == 'admin2'\").assign(\n",
    "    start_year=lambda df: df.start_date.dt.year,\n",
    "    start_month=lambda df: df.start_date.dt.month,\n",
    "    duration_in_months=lambda df: np.ceil(\n",
    "        (df.end_date - df.start_date) / np.timedelta64(1, \"M\")\n",
    "    ).astype(int),\n",
    ")\n",
    "\n",
    "admin2_outbreaks_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can merge the outbreak data with our geometries shapefile on distinct\n",
    "`location_period_id` to obtain the geometries for only our distinct admin2\n",
    "outbreak regions.  In addition, we'll drop all rows with duplicate geometries.\n",
    "This is because we have found duplicate geometries (identical coordinates) for\n",
    "different `location_period_id` values.  We must use `keep=False` to throw out\n",
    "_all_ duplicates because we don't know which row (if any) is valid.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geometries_file = \"../data/AfricaShapefiles/total_shp_0427.shp\"\n",
    "geolocations_gdf = typing.cast(\n",
    "    # We have to cast again because the pandas type hints are not properly\n",
    "    # specified for subclassing DataFrame.  Thus, after invoking various methods\n",
    "    # on our GeoDataFrame, its type reverts to DataFrame, so we must again tell\n",
    "    # the type checker that we actually have a GeoDataFrame.\n",
    "    gpd.GeoDataFrame,\n",
    "    typing.cast(gpd.GeoDataFrame, gpd.read_file(geometries_file))\n",
    "    .rename(columns={\"lctn_pr\": \"location_period_id\"})\n",
    "    .merge(\n",
    "        admin2_outbreaks_df[[\"location_period_id\"]].drop_duplicates(),\n",
    "        how=\"inner\",\n",
    "        on=\"location_period_id\",\n",
    "    )\n",
    "    .drop_duplicates(\"geometry\", keep=False)\n",
    "    .assign(location_period_id=lambda df: df[\"location_period_id\"].astype(int)),\n",
    ")\n",
    "\n",
    "geolocations_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see this gives us 473 distinct regions.  Let's visualize them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(geolocations_gdf.crs)\n",
    "geolocations_gdf.boundary.plot(linewidth=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll merge our distinct geometries with our admin2 outbreaks to see how\n",
    "many outbreaks we can work with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "admin2_outbreaks_df.merge(geolocations_gdf, how=\"inner\", on=\"location_period_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we are now left with 661 outbreaks at the admin2 level after\n",
    "eliminating all locations with duplicate geometries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll save our disctinct `location_period_id`s and geometries for use\n",
    "elsewhere:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geolocations_gdf.to_file(\"geolocations.geojson\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
