{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imblearn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn import metrics, svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.read_csv(\"../data/zonal-means-aggregate-200910-201912.csv\")\n",
    "full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = full_df[\"outbreak\"].value_counts()\n",
    "print(\"Class Distribution:\\n\", class_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SMOTE algorithm for treating imbalanced datasets cannot deal with missing values (NaNs) for Feature columns, so we need to impute the missing data. Following the methodology used by Campbell et al (2020) we will keep only those districts and months that have data for all of the environmental parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df = full_df.dropna()\n",
    "cleaned_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By removing these rows, we now have fewer outbreak (and non outbreak months) overall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = cleaned_df[\"outbreak\"].value_counts()\n",
    "print(\"Class Distribution:\\n\", class_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping unecessary columns for analysis and moving \"outbreak\" to \"y\"\n",
    "# variable as it is the feature we are trying to predict\n",
    "X_cln = cleaned_df.drop(\n",
    "    columns=[\"outbreak\", \"location_period_id\", \"month\", \"year\"]\n",
    ")  # all other columns are our feature (predicting) variables\n",
    "\n",
    "y = cleaned_df[\"outbreak\"]  # our predicted variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cln.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see explore the correlation between all of the environmental parameters we are using. By doing so, we might be able to reduce this feature space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spearman = X_cln.corr(method=\"spearman\")\n",
    "spearman.style.background_gradient(cmap=\"coolwarm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe correlation between precipitation and soil moisture values. This makes sense as one (precip) certainly has an impact on the other (soil moisture levels). We will want to consider this in our model development, as we can perhaps reduce the number of features considered. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(spearman > 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll drop those variables that have more than 0.8 correlation (i.e., we'll keep only `sm_0`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_cln.drop([\"sm_1\", \"sm_2\", \"sm_3\", \"precip_1\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spearman = X.corr(method=\"spearman\")\n",
    "spearman.style.background_gradient(cmap=\"coolwarm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into train and test splits\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accounting for an imbalanced dataset\n",
    "\n",
    "Below is a useful reference for different techniques use to solve the imbalance of classes in machine learning datasets: \n",
    "https://www.analyticsvidhya.com/blog/2020/07/10-techniques-to-deal-with-class-imbalance-in-machine-learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply SMOTE to the training data with a 1:10 ratio as used by Campbell et al 2020\n",
    "smote = SMOTE(\n",
    "    sampling_strategy=0.1, random_state=42\n",
    ")  # worked but still reflected only outbreak = 0 category\n",
    "\n",
    "# apply SMOTE at 1:2 ratio - accuracy is more reflective of minority category, but not biologically relevant\n",
    "# smote = SMOTE(sampling_strategy=0.5, random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying SMOTE\n",
    "- Behaves similarly to a data transformation object in that it must be defined and configured, fit on a dataset, then applied to create a new transformed version of the dataset.\n",
    "- In our code below, we define a SMOTE instance with default parameters that will balance the minority class and then fit and apply it in one step to create a transformed version of our dataset. The `sampling strategy=0.1` means we will `oversample` the minority class (outbreak=1) to have 10 percent number of examples of the majority class (i.e., maintain a 1:10 ratio of outbreaks to non-outbreaks).\n",
    "- Once transformed, we will expect to see the class distribution of the new transformed dataset, now to be balanced (while maintaining that 1:10 ratio) through the creation of many new synthetic examples in the minority (i.e., outbreak=1) class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the new class distribution after SMOTE\n",
    "resampled_class_counts = pd.Series(y_resampled).value_counts()\n",
    "print(\"\\nClass Distribution after SMOTE:\\n\", resampled_class_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TOMEK LINKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "# from imblearn.over_sampling import RandomOverSampler\n",
    "# from imblearn.under_sampling import TomekLinks\n",
    "\n",
    "# tl = RandomOverSampler(sampling_strategy=0.2, random_state=42)\n",
    "\n",
    "# # fit predictor and target variable\n",
    "# X_resampled, y_resampled = tl.fit_resample(X_train, y_train)\n",
    "\n",
    "# # check the new class distribution after TOMEK LINKS\n",
    "# resampled_class_counts = pd.Series(y_resampled).value_counts()\n",
    "# # print(\"\\nClass Distribution after Tomek Links:\\n\", y_tl)\n",
    "# print(\"Resampled dataset shape %s\" % Counter(y_resampled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train your machine learning model on the balanced dataset\n",
    "clf_cln = RandomForestClassifier(random_state=42)\n",
    "clf_cln.fit(X_resampled, y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate your model\n",
    "accuracy = clf_cln.score(X_test, y_test)\n",
    "print(\"\\nModel Accuracy on Test Set:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run prediction\n",
    "y_rf_pred = clf_cln.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create confusion matrix\n",
    "cnf_matrix_rf = metrics.confusion_matrix(y_test, y_rf_pred)\n",
    "cnf_matrix_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation of the confusion matrix above: \n",
    "In the first quadrant we have correctly classified 12,766 of the non-outbreak occurrences. The second and third quadrants we see 140 incorrectly classified (127 + 13) events for months where there was an outbreak. And we have correctly classified only 1 outbreak month. Our high accuracy is due to the underlying make-up of the data and it's imbalanced nature (i.e., it is classifying all non-outbreak events well as that is the predominant structure of the dataset). So we will want to revisit how we account for this imbalance in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ROCAUC score:\", metrics.roc_auc_score(y_test, y_rf_pred))\n",
    "print(\"Accuracy score:\", metrics.accuracy_score(y_test, y_rf_pred))\n",
    "print(\"F1 score:\", metrics.f1_score(y_test, y_rf_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train your machine learning model on the balanced dataset (already done in the previous code)\n",
    "\n",
    "# get feature importances from the trained RandomForestClassifier\n",
    "feature_importances = clf_cln.feature_importances_\n",
    "\n",
    "# create a DataFrame to display feature names and their corresponding importances\n",
    "feature_importance_df = pd.DataFrame(\n",
    "    {\"Feature\": X_resampled.columns, \"Importance\": feature_importances}\n",
    ")\n",
    "\n",
    "# sort the DataFrame by importance in descending order\n",
    "feature_importance_df = feature_importance_df.sort_values(\n",
    "    by=\"Importance\", ascending=False\n",
    ")\n",
    "\n",
    "# print the top N most influential features (adjust N as needed)\n",
    "top_n_features = 10  # Change this to the number of top features you want to display\n",
    "print(f\"Top {top_n_features} Most Influential Features:\")\n",
    "print(feature_importance_df.head(top_n_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clean = svm.SVC(random_state=42)\n",
    "svm_clean.fit(X_resampled, y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_svm = svm_clean.score(X_test, y_test)\n",
    "print(\"\\nModel Accuracy on Test Set:\", accuracy_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run prediction\n",
    "y_svm_pred = svm_clean.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix_svm = metrics.confusion_matrix(y_test, y_svm_pred)\n",
    "cnf_matrix_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ROCAUC score:\", metrics.roc_auc_score(y_test, y_svm_pred))\n",
    "print(\"Accuracy score:\", metrics.accuracy_score(y_test, y_svm_pred))\n",
    "print(\"F1 score:\", metrics.f1_score(y_test, y_svm_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring the same dataset using `svm` we find similar results to our Random Forest results, except that things are worse! We will need to revisit how we handle the imbalanced nature of this dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logisitic regression \n",
    "\n",
    "Here we will look at the simplest classification (logistic regression) using only the most important feature identified by the Random Forest model - to see if we can explain all outbreak months by precip in the current month alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_precip = X.drop(\n",
    "    columns=[\"lst_3\", \"lst_2\", \"lst_1\", \"lst_0\", \"precip_3\", \"precip_2\", \"sm_0\"]\n",
    ")  # keep only \"precip_0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into train and test splits\n",
    "Xp_train, Xp_test, yp_train, yp_test = train_test_split(\n",
    "    X_precip, y, test_size=0.3, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xp_resampled, yp_resampled = smote.fit_resample(Xp_train, yp_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# create an instance of the model\n",
    "logreg = LogisticRegression(solver=\"lbfgs\", max_iter=400)\n",
    "\n",
    "# train the model\n",
    "logreg.fit(Xp_resampled, yp_resampled)\n",
    "\n",
    "# run prediction\n",
    "y_pred = logreg.predict(Xp_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create confusion matrix\n",
    "from sklearn import metrics\n",
    "\n",
    "cnf_matrix = metrics.confusion_matrix(yp_test, y_pred)\n",
    "cnf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy:\", metrics.accuracy_score(yp_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo-ds-cholera",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
