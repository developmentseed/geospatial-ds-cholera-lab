{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imblearn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "\n",
    "from sklearn import metrics, svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.read_csv(\"../data/zonal-means-aggregate-200910-201912.csv\")\n",
    "full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = full_df[\"outbreak\"].value_counts()\n",
    "print(\"Class Distribution:\\n\", class_counts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SMOTE algorithm for treating imbalanced datasets cannot deal with missing values (NaNs) for Feature columns, so we need to impute the missing data. Following the methodology used by Campbell et al (2020) we will keep only those districts and months that have data for all of the environmental parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df = full_df.dropna()\n",
    "cleaned_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By removing these rows, we now have fewer outbreak (and non outbreak months) overall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = cleaned_df[\"outbreak\"].value_counts()\n",
    "print(\"Class Distribution:\\n\", class_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping unnecessary columns for analysis and moving \"outbreak\" to \"y\"\n",
    "# variable as it is the feature we are trying to predict\n",
    "X_cln = cleaned_df.drop(\n",
    "    columns=[\"outbreak\", \"location_period_id\", \"month\", \"year\"]\n",
    ")  # all other columns are our feature (predicting) variables\n",
    "\n",
    "y = cleaned_df[\"outbreak\"]  # our predicted variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cln.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removal of correlated variables\n",
    "\n",
    "A simple model is preferable. Correlated variables will not always make your model worse, but they will not always improve it. In general, it is good practice to remove correlated features because: \n",
    "* they make the algorithm learn faster\n",
    "* they decrease harmful bias (i.e., if variables are correlated with each other and not with the predicted target [e.g., outbreaks]) they may confound other interactions\n",
    "* they improve the interpretability of the model\n",
    "\n",
    "Random forest models (like the one we'll explore below) can be good at detecting interactions between different features, but highly correlated features can mask these interactions. Explore for yourself and compare results with the full dataset vs. reduced variables and see how model performance changes.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cells below, we'll explore the correlation between all of the environmental parameters we are using. By doing so, we might be able to reduce this feature space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spearman = X_cln.corr(method=\"spearman\")\n",
    "spearman.style.background_gradient(cmap=\"coolwarm\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe correlation between precipitation and soil moisture values. This makes sense as one (precip) certainly has an impact on the other (soil moisture levels). We will want to consider this in our model development, as we can perhaps reduce the number of features considered. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(spearman > 0.8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll drop those variables that have more than 0.8 correlation (i.e., we'll keep only `sm_0`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_cln.drop([\"sm_1\", \"sm_2\", \"sm_3\", \"precip_1\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spearman = X.corr(method=\"spearman\")\n",
    "spearman.style.background_gradient(cmap=\"coolwarm\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have utilized spearman rank correlation to reduce the feature space here. But there are other options as well. Correlation analyses are used extensively for variable selection, as it recognizes the degree of correlation between input and output variables. Alternatively, methods like Principal Component Analysis (PCA) can be used for identifying variables with high variances that influence the output (predicted) variable. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of training/testing datasets\n",
    "\n",
    "Normally we would want to split the entire dataset into train, validation and test datasets \n",
    "* Training set - is the portion of the dataset used to fit the model. This is what the model \"see\" and \"learn from\". It should be large enough to generate meaningful results (but not too large that overfitting occurs) and be representative of the dataset as a whole. Overfitting is when the model becomes to specialized on the training dataset that it is unable to generalize and make correct predictions on new data (i.e., the testing dataset)\n",
    "* Validation set - is used to evaluate and fine-tine the machine learning model during training, to asses the models performance and make any adjustments. \n",
    "* Testing set - It is the set of data used to evaluate the final performance of the trained model. This is the subset of data that has been hidden from the model and so allows for the evaluation of the model's performance on a real-world dataset\n",
    "\n",
    "Due to the limited number of cholera outbreaks (outbreaks=1) in the dataset, a decision was made to use only training/testing datasets with a split of 70:30 respectively. This follows similar literature methodologies used for cholera outbreak analysis (Campbell et al. 2020). \n",
    "\n",
    "The creation of the train/test dataset splits follow a random sampling approach. However, a stratified dataset splitting could (and should be) explored, especially as we are dealing with a highly imbalanced dataset. By enabling stratified splitting, we would preserve the relative proportions of each class (outbreak=1, outbreak=0) across splits. See documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) and [here](https://scikit-learn.org/stable/modules/cross_validation.html#stratification) for enabling stratification in the train/test split. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into train and test splits\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "print(X_train)\n",
    "print(y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accounting for an imbalanced dataset\n",
    "\n",
    "Below is a useful reference for different techniques use to solve the imbalance of classes in machine learning datasets: \n",
    "https://www.analyticsvidhya.com/blog/2020/07/10-techniques-to-deal-with-class-imbalance-in-machine-learning/. Additionally, here is another great resource for understanding the different methodologies used for treating imbalanced datasets: https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/.\n",
    "\n",
    "For the purposes of this investigation, we'll first start with SMOTE, followed by ADASYN-SMOTE techniques as had been suggested in the literature for similar work with imbalanced outbreak datasets. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply SMOTE to the training data with a 1:10 ratio as used by Campbell et al 2020\n",
    "smote = SMOTE(\n",
    "    sampling_strategy=0.1, random_state=42\n",
    ")  # worked but still reflected only outbreak = 0 category\n",
    "\n",
    "# apply SMOTE at 1:2 ratio - accuracy is more reflective of minority category, but not biologically relevant\n",
    "# smote = SMOTE(sampling_strategy=0.5, random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying SMOTE\n",
    "- Behaves similarly to a data transformation object in that it must be defined and configured, fit on a dataset, then applied to create a new transformed version of the dataset.\n",
    "- In our code below, we define a SMOTE instance with default parameters that will balance the minority class and then fit and apply it in one step to create a transformed version of our dataset. The `sampling strategy=0.1` means we will `oversample` the minority class (outbreak=1) to have 10 percent number of examples of the majority class (i.e., maintain a 1:10 ratio of outbreaks to non-outbreaks).\n",
    "- Once transformed, we will expect to see the class distribution of the new transformed dataset, now to be balanced (while maintaining that 1:10 ratio) through the creation of many new synthetic examples in the minority (i.e., outbreak=1) class."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADASYN SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oversample = ADASYN(sampling_strategy=0.1, random_state=42)\n",
    "# X_resampled, y_resampled = oversample.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the new class distribution after SMOTE\n",
    "resampled_class_counts = pd.Series(y_resampled).value_counts()\n",
    "print(\"\\nClass Distribution after SMOTE:\\n\", resampled_class_counts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensitivity analysis of SMOTE vs ADASYN SMOTE treatments"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sensitivity analysis of SMOTE imbalance treatments (oversampling of the minority class [outbreaks = 1]). This was performed by running the Random Forest model and evaluating on the test dataset, while changing only the sampling strategy parameter value. \n",
    "\n",
    "| SMOTE Sampling strategy parameter | Accuracy | F1 Score | ROC AUC Score | \n",
    "|-----------------------------|----------|----------|--------------|\n",
    "| 0.1 | 0.989 | 0.014 | 0.503|\n",
    "| 0.2 | 0.986 | 0.044 | 0.512|\n",
    "| 0.3 | 0.984 | 0.063 | 0.524| \n",
    "| 0.4 | 0.981 | 0.056 | 0.522|\n",
    "| 0.5 | 0.979 | 0.069 | 0.533| \n",
    "\n",
    "With the lower (0.1) sampling strategy we get our highest accuracy scores, but that is reflective of correctly predicting the majority class (outbreaks = 0) only. A 1:2 sampling strategy allows us to correctly predict cholera outbreaks, while maintaining decent accuracy scores - however it is not biologically relevant in the real world (i.e., there isn't a 1:2 ratio of outbreak to non-outbreak months). \n",
    "\n",
    "| SMOTE ADASYN Sampling strategy parameter | Accuracy | F1 Score | ROC AUC Score | \n",
    "|-----------------------------|----------|----------|--------------|\n",
    "| 0.1 | 0.989 | 0.042 | 0.511|\n",
    "| 0.2 | 0.987 | 0.067 | 0.521|\n",
    "| 0.3 | 0.983 | 0.062 | 0.523| \n",
    "| 0.4 | 0.981 | 0.056 | 0.522|\n",
    "| 0.5 | 0.979 | 0.069 | 0.533| \n",
    "\n",
    "We also observe that both SMOTE vs ADASYN SMOTE techniques result in very similar (nearly negligible differences in) model results. \n",
    "\n",
    "## Evaluation metrics\n",
    "\n",
    "**Accuracy**: Represents the total number of correctly classified data instances over the total number of data instance. This metric alone would is not a good measure if the dataset is highly imbalanced (as we have seen in the example above the high accuracy scores are a reflection of the majority class only). \n",
    "\n",
    "**F1 Score**: F1 score gives a single metrics that balances both precision and recall. It can be used, in a complementary manner with ROC AUC scores, to assess the effectiveness of a ML model. An F1 score is high (1) only when both the precision and recall are both high. \n",
    "\n",
    "**ROC AUC Score**: Tells us how efficient the model is. A higher AUC the better the model's performance at distinguishing between positive and negative classes. A score of 1 means the classifier can perfectly distinguish between positive and negative classes. An AUC value of 0 means the classifier predicts all negatives as positives and vice versa. A ROC AUC of 0.5 means the classifier is not working. An AUC value above 0.5 means the classifier can detect more numbers of true positives and true negatives than false positives and false negatives. \n",
    "\n",
    "For more information on evaluation metrics, please see this [helpful resource](https://medium.com/analytics-vidhya/confusion-matrix-accuracy-precision-recall-f1-score-ade299cf63cd). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of stratified training/testing datsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into train and test splits with stratify set\n",
    "(\n",
    "    stratified_X_train,\n",
    "    stratified_X_test,\n",
    "    stratified_y_train,\n",
    "    stratified_y_test,\n",
    ") = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "print(stratified_X_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application of SMOTE on stratified split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply SMOTE to the training data with a 1:10 ratio as used by Campbell et al 2020\n",
    "smote = SMOTE(\n",
    "    sampling_strategy=0.1, random_state=42\n",
    ")  # worked but still reflected only outbreak = 0 category\n",
    "\n",
    "# apply SMOTE at 1:2 ratio - accuracy is more reflective of minority category, but not biologically relevant\n",
    "# smote = SMOTE(sampling_strategy=0.5, random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application of ADASYN SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oversample = ADASYN(sampling_strategy=0.1, random_state=42)\n",
    "# X_resampled, y_resampled = oversample.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the new class distribution after SMOTE\n",
    "resampled_class_counts = pd.Series(y_resampled).value_counts()\n",
    "print(\"\\nClass Distribution after SMOTE:\\n\", resampled_class_counts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See also (TOMEK LINKS Exploration) \n",
    "Was also explored, but didn't work as successfully as either of the approaches above. Saved here for archived methodology, otherwise ignore. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "\n",
    "tl = TomekLinks()\n",
    "\n",
    "# tl = RandomOverSampler(sampling_strategy=0.2, random_state=42)\n",
    "\n",
    "# fit predictor and target variable\n",
    "X_resampled, y_resampled = tl.fit_resample(X_train, y_train)\n",
    "\n",
    "# check the new class distribution after TOMEK LINKS\n",
    "resampled_class_counts = pd.Series(y_resampled).value_counts()\n",
    "\n",
    "# print(\"\\nClass Distribution after Tomek Links:\\n\", y_tl)\n",
    "print(f\"TomekLinks Resampled dataset shape {Counter(y_resampled)}\")\n",
    "\n",
    "# TomekLinks Resampled dataset shape Counter({0: 29585, 1: 358})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTEENN Exploration\n",
    "Was also explored, but didn't work as successfully as either of the approaches above. Saved here for archived methodology, otherwise ignore. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tl = SMOTEENN(sampling_strategy=0.2, random_state=42)\n",
    "\n",
    "# Resampled dataset shape Counter({0: 29758, 1: 5951})\n",
    "# tl = RandomOverSampler(sampling_strategy=0.2, random_state=42)\n",
    "\n",
    "# fit predictor and target variable\n",
    "# X_resampled, y_resampled = tl.fit_resample(X_train, y_train)\n",
    "\n",
    "# check the new class distribution after TOMEK LINKS\n",
    "# resampled_class_counts = pd.Series(y_resampled).value_counts()\n",
    "\n",
    "# print(\"\\nClass Distribution after Tomek Links:\\n\", y_tl)\n",
    "# print(f'SMOTEEN Resampled dataset shape {Counter(y_resampled)}')\n",
    "# SMOTTEENN Resampled dataset shape Counter({0: 26028, 1: 5031})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTETomek Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "# SMOTETomek Resampled dataset shape Counter({0: 29601, 1: 5794})\n",
    "# SMOTTEENN Resampled dataset shape Counter({0: 26028, 1: 5031})\n",
    "# tl = SMOTETomek(sampling_strategy=0.2, random_state=42)\n",
    "\n",
    "# Resampled dataset shape Counter({0: 29758, 1: 5951})\n",
    "# tl = RandomOverSampler(sampling_strategy=0.2, random_state=42)\n",
    "\n",
    "# fit predictor and target variable\n",
    "# X_resampled, y_resampled = tl.fit_resample(X_train, y_train)\n",
    "\n",
    "# check the new class distribution after TOMEK LINKS\n",
    "# resampled_class_counts = pd.Series(y_resampled).value_counts()\n",
    "\n",
    "# print(\"\\nClass Distribution after Tomek Links:\\n\", y_tl)\n",
    "# print(f'SMOTETomek Resampled dataset shape {Counter(y_resampled)}')\n",
    "# SMOTETomek Resampled dataset shape Counter({0: 29601, 1: 5794})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Exploration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train your machine learning model on the balanced dataset\n",
    "clf_cln = RandomForestClassifier(random_state=42)\n",
    "clf_cln.fit(X_resampled, y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate your model\n",
    "accuracy = clf_cln.score(X_test, y_test)\n",
    "# (prev) Model Accuracy on Test Set: 0.9890756953591074\n",
    "print(\"\\nModel Accuracy on Test Set:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run prediction\n",
    "y_rf_pred = clf_cln.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create confusion matrix\n",
    "cnf_matrix_rf = metrics.confusion_matrix(y_test, y_rf_pred)\n",
    "cnf_matrix_rf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix\n",
    "| | predicted condition |\n",
    "|--|--|\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation of the confusion matrix above: \n",
    "In the first quadrant we have correctly classified 12,766 of the non-outbreak occurrences. The second and third quadrants we see 140 incorrectly classified (127 + 13) events for months where there was an outbreak. And we have correctly classified only 1 outbreak month. Our high accuracy is due to the underlying make-up of the data and it's imbalanced nature (i.e., it is classifying all non-outbreak events well as that is the predominant structure of the dataset). So we will want to revisit how we account for this imbalance in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ROC AUC score:\", metrics.roc_auc_score(y_test, y_rf_pred))\n",
    "print(\"Accuracy score:\", metrics.accuracy_score(y_test, y_rf_pred))\n",
    "print(\"F1 score:\", metrics.f1_score(y_test, y_rf_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train your machine learning model on the balanced dataset (already done in the previous code)\n",
    "\n",
    "# get feature importances from the trained RandomForestClassifier\n",
    "feature_importances = clf_cln.feature_importances_\n",
    "\n",
    "# create a DataFrame to display feature names and their corresponding importances\n",
    "feature_importance_df = pd.DataFrame(\n",
    "    {\"Feature\": X_resampled.columns, \"Importance\": feature_importances}\n",
    ")\n",
    "\n",
    "# sort the DataFrame by importance in descending order\n",
    "feature_importance_df = feature_importance_df.sort_values(\n",
    "    by=\"Importance\", ascending=False\n",
    ")\n",
    "\n",
    "# print the top N most influential features (adjust N as needed)\n",
    "top_n_features = 10  # Change this to the number of top features you want to display\n",
    "print(f\"Top {top_n_features} Most Influential Features:\")\n",
    "print(feature_importance_df.head(top_n_features))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines\n",
    "Was also explored, but was not as successful as Random Forest. Methodology saved below for archive and comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clean = svm.SVC(random_state=42)\n",
    "svm_clean.fit(X_resampled, y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_svm = svm_clean.score(X_test, y_test)\n",
    "print(\"\\nModel Accuracy on Test Set:\", accuracy_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run prediction\n",
    "y_svm_pred = svm_clean.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix_svm = metrics.confusion_matrix(y_test, y_svm_pred)\n",
    "cnf_matrix_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ROC AUC score:\", metrics.roc_auc_score(y_test, y_svm_pred))\n",
    "print(\"Accuracy score:\", metrics.accuracy_score(y_test, y_svm_pred))\n",
    "print(\"F1 score:\", metrics.f1_score(y_test, y_svm_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring the same dataset using `svm` we find similar results to our Random Forest results, except that things are worse! We will need to revisit how we handle the imbalanced nature of this dataset. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logisitic regression \n",
    "\n",
    "Here we will look at the simplest classification (logistic regression) using only the most important feature identified by the Random Forest model - to see if we can explain all outbreak months by precip in the current month alone. We will explore this as a siml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_precip = X.drop(\n",
    "    columns=[\"lst_3\", \"lst_2\", \"lst_1\", \"lst_0\", \"precip_3\", \"precip_2\", \"sm_0\"]\n",
    ")  # keep only \"precip_0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into train and test splits\n",
    "Xp_train, Xp_test, yp_train, yp_test = train_test_split(\n",
    "    X_precip, y, test_size=0.3, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xp_resampled, yp_resampled = smote.fit_resample(Xp_train, yp_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# create an instance of the model\n",
    "logreg = LogisticRegression(solver=\"lbfgs\", max_iter=400)\n",
    "\n",
    "# train the model\n",
    "logreg.fit(Xp_resampled, yp_resampled)\n",
    "\n",
    "# run prediction\n",
    "y_pred = logreg.predict(Xp_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create confusion matrix\n",
    "from sklearn import metrics\n",
    "\n",
    "cnf_matrix = metrics.confusion_matrix(yp_test, y_pred)\n",
    "cnf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ROC AUC score:\", metrics.roc_auc_score(yp_test, y_pred))\n",
    "print(\"Accuracy score:\", metrics.accuracy_score(yp_test, y_pred))\n",
    "print(\"F1 score:\", metrics.f1_score(yp_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Findings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The input cholera dataset was largely imbalanced. As such, the model accuracy largely reflects the underlying class distribution.\n",
    "* Dataset imbalances have been identified as a key challenge in accompanying ML approaches, including [cholera outbreak analysis](https://www.mdpi.com/1660-4601/17/24/9378)\n",
    "* Synthetic Minority Oversampling Technique (SMOTE) use during the pre-processing stage allows for the generation of new examples of the minority class (outbreak=1)\n",
    "* Different imbalance ratios were explored during a sensitivity analysis, with more balanced datasets producing higher accuracy results\n",
    "* However, assuming a 1:1 ratio of outbreaks vs. non-outbreaks is unrealistic in real-world data\n",
    "* Thus far a SMOTE sampling strategy of 0.3 proved to provide a reasonable balance between ML requirements and realistic applications to cholera outbreak analysis with an accuracy score of 0.984, F1 score of 0.063, and ROC AUC of of 0.524.  \n",
    "* Further exploration of treatments for imbalanced datasets, reduction of environmental parameters within the random forest model should be pursued. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo-ds-cholera",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
